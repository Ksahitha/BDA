{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Natural Numbers\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers = list(range(1, 16))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "#Produce RDD with List of first 15 natural numbers\n",
        "print(\"Elements in the RDD:\", rdd.collect())\n",
        "\n",
        "# Show the number of partitions in the RDD\n",
        "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC634Yl_bdZ-",
        "outputId": "b40d7bfa-5fb9-45a8-a28a-042d764ed2ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elements in the RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "Number of partitions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Natural Numbers\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers = list(range(1, 16))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# Get the first element of the RDD\n",
        "first_element = rdd.first()\n",
        "\n",
        "# Show the first element\n",
        "print(\"First element in the RDD:\", first_element)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ70NDzKcBMY",
        "outputId": "654acd5f-40de-482e-82c5-0da1b8b04f25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First element in the RDD: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Filter Even Numbers\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers = list(range(1, 16))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# Use the filter transformation to select only even numbers\n",
        "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Collect and print the filtered RDD to see the result\n",
        "print(\"Even numbers in the RDD:\", even_rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbw38MLucTo6",
        "outputId": "753a3909-7c15-4902-dde1-c226a36dded6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Even numbers in the RDD: [2, 4, 6, 8, 10, 12, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Square Each Element\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers = list(range(1, 16))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# Use the map transformation to square each element\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)\n",
        "\n",
        "# Collect and print the transformed RDD\n",
        "print(\"Squared numbers in the RDD:\", squared_rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_iiKprhcl8r",
        "outputId": "caa4898f-1559-44b8-a3e6-4a9bf5740ee3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared numbers in the RDD: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Reduce Example\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers = list(range(1, 16))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# Use the reduce action to calculate the sum of all elements\n",
        "sum_result = rdd.reduce(lambda x, y: x + y)\n",
        "\n",
        "# Print the result\n",
        "print(\"Sum of all elements in the RDD:\", sum_result)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKjieadUcxeP",
        "outputId": "58831beb-41fc-4a05-ddb9-5be29b578237"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of all elements in the RDD: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Save RDD as Text File\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers = list(range(1, 16))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# Save the RDD as a text file\n",
        "rdd.saveAsTextFile(\"output/rdd_numbers.txt\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "RpVwNOuLdAM7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Union Example\")\n",
        "\n",
        "# Create two lists of numbers\n",
        "list1 = [1, 2, 3, 4, 5]\n",
        "list2 = [6, 7, 8, 9, 10]\n",
        "\n",
        "# Parallelize the lists to create RDDs\n",
        "rdd1 = sc.parallelize(list1)\n",
        "rdd2 = sc.parallelize(list2)\n",
        "\n",
        "# Use the union transformation to combine the two RDDs\n",
        "combined_rdd = rdd1.union(rdd2)\n",
        "\n",
        "# Collect and print the combined RDD\n",
        "print(\"Combined RDD:\", combined_rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WoKd6QEdM_E",
        "outputId": "fc3a79b8-e60b-443c-c7b5-0dc796389f2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Cartesian Example\")\n",
        "\n",
        "# Create two lists of numbers\n",
        "list1 = [1, 2]\n",
        "list2 = [3, 4]\n",
        "\n",
        "# Parallelize the lists to create RDDs\n",
        "rdd1 = sc.parallelize(list1)\n",
        "rdd2 = sc.parallelize(list2)\n",
        "\n",
        "# Use the cartesian transformation to get all ordered pairs\n",
        "cartesian_rdd = rdd1.cartesian(rdd2)\n",
        "\n",
        "# Collect and print the Cartesian product RDD\n",
        "print(\"Cartesian product RDD:\", cartesian_rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ1bx5XYdcHG",
        "outputId": "0dbc5b58-8f34-4d3b-ed84-1ff29a921a2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cartesian product RDD: [(1, 3), (1, 4), (2, 3), (2, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Dictionary RDD\")\n",
        "\n",
        "# Create a dictionary\n",
        "data_dict = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
        "\n",
        "# Convert the dictionary to a list of tuples (key-value pairs)\n",
        "data_list = list(data_dict.items())\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(data_list)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(\"RDD with dictionary data:\", rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csYhv7YhdjOZ",
        "outputId": "c83af829-50f2-4e9e-bccb-ff1f9335bd55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD with dictionary data: [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Count Unique Values\")\n",
        "\n",
        "# Create a list of numbers (with some repeated values)\n",
        "numbers = [1, 2, 3, 2, 1, 1, 4, 5, 2, 3, 5]\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# Use map transformation to create key-value pairs (value, 1)\n",
        "rdd_pairs = rdd.map(lambda x: (x, 1))\n",
        "\n",
        "# Use reduceByKey to aggregate counts for each unique value\n",
        "counted_rdd = rdd_pairs.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect and print the results\n",
        "print(\"Unique values and their counts:\", counted_rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INveshnTdxgC",
        "outputId": "47da9a7a-5f67-4ea5-ea56-7589e777ef9c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values and their counts: [(1, 3), (2, 3), (3, 2), (4, 1), (5, 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "7kCmRO5FkOt4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Save RDD Multiple Text Files\")\n",
        "\n",
        "# Create a list of the first 15 natural numbers\n",
        "numbers1 = list(range(1, 16))\n",
        "numbers2 = list(range(16, 31))\n",
        "\n",
        "# Parallelize the list to create an RDD\n",
        "rdd1 = sc.parallelize(numbers1)\n",
        "rdd2 = sc.parallelize(numbers2)\n",
        "\n",
        "# Save the RDD as a text file\n",
        "rdd1.saveAsTextFile(\"output/rdd_numbers1.txt\")\n",
        "rdd2.saveAsTextFile(\"output/rdd_numbers2.txt\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "hatHYyLnkt_N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Combine Multiple Text Files\")\n",
        "\n",
        "# Load all .txt files from the \"input_files\" directory into a single RDD\n",
        "rdd = sc.textFile(\"output/*.txt\")\n",
        "\n",
        "# Collect and print the RDD contents\n",
        "print(\"Combined RDD from multiple files:\")\n",
        "for line in rdd.collect():\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHom4vVylTkJ",
        "outputId": "7d1d433f-4237-42f8-b24f-004a3e5997fc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined RDD from multiple files:\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "_VNGL_fimd_B"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Inspect First 5 Lines of RDD\")\n",
        "\n",
        "# Create an example RDD (e.g., a list of lines)\n",
        "rdd = sc.parallelize([\"Line 1\", \"Line 2\", \"Line 3\", \"Line 4\", \"Line 5\", \"Line 6\", \"Line 7\"])\n",
        "\n",
        "# Use the take() action to get the first 5 lines\n",
        "first_5_lines = rdd.take(5)\n",
        "\n",
        "# Print the first 5 lines\n",
        "print(\"First 5 lines of the RDD:\")\n",
        "for line in first_5_lines:\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnU1lgvmmZLS",
        "outputId": "1b2bf361-2bd2-4146-a62c-14c6fbea01be"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 lines of the RDD:\n",
            "Line 1\n",
            "Line 2\n",
            "Line 3\n",
            "Line 4\n",
            "Line 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"Spark DataFrame Example\").getOrCreate()\n",
        "\n",
        "# Example data as a list of tuples\n",
        "data = [(\"Alice\", 25, \"New York\"),\n",
        "        (\"Bob\", 30, \"Los Angeles\"),\n",
        "        (\"Charlie\", 35, \"Chicago\")]\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"City\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ZjUBFqwPDc",
        "outputId": "9315c5d8-31a5-4f1a-f531-3507488801e6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----------+\n",
            "|   Name|Age|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice| 25|   New York|\n",
            "|    Bob| 30|Los Angeles|\n",
            "|Charlie| 35|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 RDD\n",
        "  #The most basic form of data in Spark\n",
        "  #No structure, so you have full control, but it’s harder to work with and slower\n",
        "#2 DataFrame\n",
        "  #A more structured form of data with column names and types\n",
        "  #Faster than RDDs because it’s optimized by Spark’s engine\n",
        "#3 Dataset\n",
        "  #Like a DataFrame but with type safety (only in Scala/Java)\n",
        "  #Both performance optimization and strong datatype"
      ],
      "metadata": {
        "id": "S65P54Liyh_m"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}